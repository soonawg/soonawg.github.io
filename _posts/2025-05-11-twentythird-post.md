---
title: "MBRL에 대해 알아보자"
layout: post
date: 2025-05-11 16:15:00 +0900
categories: study
---

# MBRL에 대해서 알아보자.
MBRL(Model-Based RL)에 대해서 알기위해서는 Model-Free RL의 차이에 대해서 알고가야할 필요가 있다.
Model-Free RL은 환경과 직접 상호작용을 하며 policy/value만 학습을 한다. 예를 들어서, PPO, DQN 등이 있다.
그에 반해, MBRL은 환경의 dynamics를 추정하는 모델을 먼저 학습하고, 그 모델을 기반으로 policy를 학습한다.
간단히 말해 환경을 모방하는 가상 모델을 만들고 그 모델을 기반으로 policy를 만든다고 보면된다.

## 그렇다면 왜 굳이 MBRL을 사용해야할까?
왜냐면 그냥 환경을 이용해서 학습하면 되는거 아니냐는 생각이 들 수도 있지않나.
그 이유는 다음과 같다.
1. 샘플 효율성 증가 : 진짜 환경이 아니라 가상 환경 모델로 더 많은 데이터를 생성해낼 수 있다.
2. 계획 수립 가능 : dynamics model이 있으니 미래 상태를 예측이 가능하다.
3. 학습 안정성 : 잘 만든 모델이면 적은 데이터로도 더 좋은 정책을 얻을 수 있다.

## 주요 MBRL 알고리즘
1. PlaNet
2. Dreamer   

Dreamer가 PlaNet보다 발전된 알고리즘.

## Dreamer의 간단 학습 흐름 개요
Dreamer의 핵심 역시 현실 환경을 흉내 낼 수 있는 가짜 환경 모델을 먼저 학습하는 것이다.
간단히 정리하자면,
1. 현실 환경에서의 경험을 수집한다 (메모리 버퍼를 사용)
2. 그 경험을 이용해 Model을 학습한다.
3. 그리고 현실 환경은 우선 꺼두고, 학습한 모델 안에서 상상으로 rollout을 시작한다,
4. 생성된 trajectory를 통해 policy/value를 학습한다.